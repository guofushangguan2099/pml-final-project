# src/train.py

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import pandas as pd
import pickle
import time
import math

# --- å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ¨¡å— ---
from model import Encoder, Attention, Decoder, Seq2Seq
from config import (
    DATA_PATH, MODEL_SAVE_PATH, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS,
    DROPOUT, BATCH_SIZE, N_EPOCHS, LEARNING_RATE, TEACHER_FORCING_RATIO,
    CLIP, DEVICE, PRINT_EVERY
)

# --- 1. æ•°æ®é›†ç±»å’Œæ•°æ®åŠ è½½å™¨ ---
from new_preprocess import Vocabulary
class TranslationDataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        src = torch.tensor(self.df.iloc[idx]['modern_numerical'], dtype=torch.long)
        trg = torch.tensor(self.df.iloc[idx]['shakespearean_numerical'], dtype=torch.long)
        return src, trg

def create_collate_fn(pad_idx):
    def collate_fn(batch):
        src_batch, trg_batch = [], []
        for src_sample, trg_sample in batch:
            src_batch.append(src_sample)
            trg_batch.append(trg_sample)
        
        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_idx)
        trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=pad_idx)
        return src_padded, trg_padded
    return collate_fn

# --- 2. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ---

def train_fn(model, dataloader, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    for i, (src, trg) in enumerate(dataloader):
        src, trg = src.to(DEVICE), trg.to(DEVICE)
        
        optimizer.zero_grad()
        output = model(src, trg, teacher_forcing_ratio=TEACHER_FORCING_RATIO)
        
        # trg = [batch_size, trg_len]
        # output = [batch_size, trg_len, output_dim]
        output_dim = output.shape[-1]
        
        # è°ƒæ•´å½¢çŠ¶ä»¥è®¡ç®—æŸå¤± (å»æ‰<sos>æ ‡è®°)
        output = output[:, 1:].reshape(-1, output_dim)
        trg = trg[:, 1:].reshape(-1)
        
        loss = criterion(output, trg)
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        
        epoch_loss += loss.item()
        
        if (i + 1) % PRINT_EVERY == 0:
            print(f"  Batch {i+1}/{len(dataloader)} | Loss: {loss.item():.4f}")

    return epoch_loss / len(dataloader)

def evaluate_fn(model, dataloader, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for i, (src, trg) in enumerate(dataloader):
            src, trg = src.to(DEVICE), trg.to(DEVICE)
            
            # å…³é—­æ•™å¸ˆå¼ºåˆ¶è¿›è¡Œè¯„ä¼°
            output = model(src, trg, teacher_forcing_ratio=0) 
            
            output_dim = output.shape[-1]
            output = output[:, 1:].reshape(-1, output_dim)
            trg = trg[:, 1:].reshape(-1)
            
            loss = criterion(output, trg)
            epoch_loss += loss.item()
            
    return epoch_loss / len(dataloader)

# --- è¾…åŠ©å‡½æ•° ---
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

# --- 3. ä¸»è®­ç»ƒæµç¨‹ ---
def run_training():
    print("--- Starting Training Process ---")
    print(f"Device: {DEVICE}")
    
    # 1. åŠ è½½æ•°æ®ä¸è¯æ±‡è¡¨
    print("Loading data and vocabularies...")
    df = pd.read_pickle(DATA_PATH)
    
    with open('data/modern_vocab.pkl', 'rb') as f:
        modern_vocab = pickle.load(f)
    with open('data/shakespearean_vocab.pkl', 'rb') as f:
        shakespearean_vocab = pickle.load(f)
        
    # æ ¹æ®è¯æ±‡è¡¨è®¾ç½®è¾“å…¥è¾“å‡ºç»´åº¦
    INPUT_DIM = modern_vocab.n_words
    OUTPUT_DIM = shakespearean_vocab.n_words
    PAD_IDX = modern_vocab.word2idx['<pad>']
    
    print(f"Vocab Sizes - Input: {INPUT_DIM}, Output: {OUTPUT_DIM}")

    # 2. æ•°æ®åˆ’åˆ† (70% Train, 15% Val, 15% Test)
    #å…ˆæ‰“ä¹±æ•°æ®ï¼Œç¡®ä¿éšæœºæ€§
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    n_total = len(df)
    n_train = int(n_total * 0.70)
    n_val = int(n_total * 0.15)
    # å‰©ä¸‹çš„ç»™æµ‹è¯•é›†
    
    train_df = df.iloc[:n_train]
    valid_df = df.iloc[n_train : n_train + n_val]
    test_df = df.iloc[n_train + n_val :]
    
    print(f"\nData Split Result:")
    print(f"  Training Set:   {len(train_df)} samples")
    print(f"  Validation Set: {len(valid_df)} samples")
    print(f"  Test Set:       {len(test_df)} samples")

    # 3. åˆ›å»º DataLoader
    train_dataset = TranslationDataset(train_df)
    valid_dataset = TranslationDataset(valid_df)
    test_dataset = TranslationDataset(test_df) # æ–°å¢æµ‹è¯•é›†

    collate_fn = create_collate_fn(PAD_IDX)
    
    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)
    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)

    # 4. åˆå§‹åŒ–æ¨¡å‹
    print("\nInitializing model...")
    attn = Attention(HIDDEN_DIM)
    enc = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)
    dec = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT, attn)
    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)
    
    # åˆå§‹åŒ–å‚æ•°æƒé‡ (æ¨èæ­¥éª¤ï¼Œæœ‰åŠ©äºæ¨¡å‹æ”¶æ•›)
    def init_weights(m):
        for name, param in m.named_parameters():
            if 'weight' in name:
                nn.init.normal_(param.data, mean=0, std=0.01)
            else:
                nn.init.constant_(param.data, 0)
    model.apply(init_weights)
    print(f"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.")

    # 5. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    # âœ… å…³é”®ä¿®æ”¹ï¼šæ·»åŠ  weight_decay=1e-5 è¿›è¡Œæ­£åˆ™åŒ–
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

    # 6. è®­ç»ƒå¾ªç¯ (å«æ—©åœæœºåˆ¶)
    best_valid_loss = float('inf')
    PATIENCE = 3      # âœ… æ—©åœè€å¿ƒå€¼ï¼šå¦‚æœéªŒè¯é›†3æ¬¡æ²¡æœ‰å˜å¥½å°±åœæ­¢
    patience_counter = 0
    
    print("\n--- Starting Epochs (with Early Stopping) ---")
    for epoch in range(N_EPOCHS):
        start_time = time.time()
        
        print(f"Epoch {epoch+1}/{N_EPOCHS}")
        
        # è®­ç»ƒå’ŒéªŒè¯
        train_loss = train_fn(model, train_dataloader, optimizer, criterion, CLIP)
        valid_loss = evaluate_fn(model, valid_dataloader, criterion)
        
        end_time = time.time()
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)
        
        # è®¡ç®— PPL
        train_ppl = math.exp(train_loss)
        val_ppl = math.exp(valid_loss)
        
        # --- æ—©åœä¸æ¨¡å‹ä¿å­˜é€»è¾‘ ---
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            patience_counter = 0 # é‡ç½®è®¡æ•°å™¨
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            save_msg = "âœ… Model Saved (New Best)"
        else:
            patience_counter += 1
            save_msg = f"âš ï¸ No Improvement ({patience_counter}/{PATIENCE})"
        
        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | {save_msg}')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {val_ppl:7.3f}')
        
        # è§¦å‘æ—©åœ
        if patience_counter >= PATIENCE:
            print(f"\nğŸ›‘ Early stopping triggered! Best validation loss was {best_valid_loss:.3f}")
            break

    # 7. æœ€ç»ˆæµ‹è¯• (Test Set Evaluation)
    print("\n--- Training Finished. Evaluating on Independent Test Set ---")
    # åŠ è½½ä¿å­˜çš„æœ€ä½³æ¨¡å‹ (é˜²æ­¢ä½¿ç”¨çš„æ˜¯æœ€åä¸€æ¬¡è¿‡æ‹Ÿåˆçš„å‚æ•°)
    model.load_state_dict(torch.load(MODEL_SAVE_PATH))
    
    test_loss = evaluate_fn(model, test_dataloader, criterion)
    test_ppl = math.exp(test_loss)
    
    print(f"{'='*40}")
    print(f"FINAL TEST RESULTS")
    print(f"{'='*40}")
    print(f"Test Loss: {test_loss:.3f}")
    print(f"Test PPL:  {test_ppl:.3f}")
    print(f"{'='*40}")
    
    return model


if __name__ == '__main__':
    run_training()
